{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "LittleVgg.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlWvvTIo3_Pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75wff1U2F9PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loohXmHp3_QH",
        "colab_type": "code",
        "colab": {},
        "outputId": "29279f57-ed6c-4798-9021-81483bff75cc"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# SET BELOW TO TRAINING SET DIRECTORY\n",
        "train_data_dir = './Full_Set - Copy'\n",
        "\n",
        "img_rows, img_cols = 448, 448\n",
        "\n",
        "#SET num_classes to the number of different classes you are training on\n",
        "num_classes = 3\n",
        "batch_size = 16\n",
        "\n",
        "# Below creates datagenerator with augmentations set \n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=45,\n",
        "      width_shift_range=0.3,\n",
        "      height_shift_range=0.3,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest',\n",
        "      validation_split = 0.2)\n",
        " \n",
        " \n",
        "# set our batch size (typically on most mid tier systems we'll use 16-32)\n",
        "batch_size = 16\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset = 'training')\n",
        " \n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset = 'validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 857 images belonging to 3 classes.\n",
            "Found 213 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcNfXb253_QR",
        "colab_type": "code",
        "colab": {},
        "outputId": "a2b42d49-6f5d-48b0-b034-ed97fc3c17c7"
      },
      "source": [
        "train_generator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.image.directory_iterator.DirectoryIterator at 0x2625217ebc8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3BvAMz83_Qe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7_WsDlJ3_Ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import ELU\n",
        "from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq6cP4RD3_Qv",
        "colab_type": "code",
        "colab": {},
        "outputId": "2b1a9af8-55db-47b6-df33-6fc37c4e5c33"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
        "                 input_shape = (img_rows, img_cols, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
        "                 input_shape = (img_rows, img_cols, 3)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
        "# layer set\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
        "# layer set\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
        "# layer set\n",
        "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Block #5: first set of FC => RELU layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Block #6: second set of FC => RELU layers\n",
        "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Block #7: softmax classifier\n",
        "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 448, 448, 32)      896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 448, 448, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 448, 448, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 448, 448, 32)      9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 448, 448, 32)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 448, 448, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 224, 224, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 224, 224, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 224, 224, 64)      18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 224, 224, 64)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 224, 224, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 224, 224, 64)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 224, 224, 64)      256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 112, 112, 128)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 112, 112, 128)     512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 112, 112, 128)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 112, 112, 128)     512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 56, 56, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 56, 56, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 56, 56, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 56, 56, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 200704)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                12845120  \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 195       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 14,026,083\n",
            "Trainable params: 14,023,907\n",
            "Non-trainable params: 2,176\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKzNpIaV3_Q9",
        "colab_type": "code",
        "colab": {},
        "outputId": "e4354952-d9d9-4e0b-f5b0-5c06f02948c5"
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "\n",
        "plot_model(model, to_file='LittleVGG.png', show_shapes=True, show_layer_names=True)\n",
        "# img = mpimg.imread('LittleVGG.png')\n",
        "# plt.figure(figsize=(100,70))\n",
        "# imgplot = plt.imshow(img) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O3b0ejk3_RB",
        "colab_type": "code",
        "colab": {},
        "outputId": "24bea71c-b6bf-4879-f1f2-ab8fe1c3e568"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "#Change below to directory to store models\n",
        "checkpoint = ModelCheckpoint(\"C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 8,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\n",
        "\n",
        "# We use a very small learning rate \n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = Adam(lr=0.001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "nb_train_samples = 833\n",
        "nb_validation_samples = 220 \n",
        "epochs = 50\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 52 steps, validate for 13 steps\n",
            "Epoch 1/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 1.5832 - accuracy: 0.3923\n",
            "Epoch 00001: val_loss improved from inf to 11.71129, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 99s 2s/step - loss: 1.5823 - accuracy: 0.3919 - val_loss: 11.7113 - val_accuracy: 0.3750\n",
            "Epoch 2/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 1.2895 - accuracy: 0.4682\n",
            "Epoch 00002: val_loss improved from 11.71129 to 1.87555, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 97s 2s/step - loss: 1.2854 - accuracy: 0.4713 - val_loss: 1.8755 - val_accuracy: 0.4567\n",
            "Epoch 3/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 1.1119 - accuracy: 0.5318\n",
            "Epoch 00003: val_loss improved from 1.87555 to 1.37393, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 97s 2s/step - loss: 1.1125 - accuracy: 0.5336 - val_loss: 1.3739 - val_accuracy: 0.5192\n",
            "Epoch 4/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 1.1384 - accuracy: 0.5579\n",
            "Epoch 00004: val_loss improved from 1.37393 to 1.03266, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 97s 2s/step - loss: 1.1364 - accuracy: 0.5568 - val_loss: 1.0327 - val_accuracy: 0.5673\n",
            "Epoch 5/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 1.0213 - accuracy: 0.5878\n",
            "Epoch 00005: val_loss did not improve from 1.03266\n",
            "52/52 [==============================] - 96s 2s/step - loss: 1.0183 - accuracy: 0.5873 - val_loss: 1.1356 - val_accuracy: 0.5913\n",
            "Epoch 6/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 0.9633 - accuracy: 0.6164\n",
            "Epoch 00006: val_loss did not improve from 1.03266\n",
            "52/52 [==============================] - 96s 2s/step - loss: 0.9657 - accuracy: 0.6142 - val_loss: 1.1940 - val_accuracy: 0.5337\n",
            "Epoch 7/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 0.8638 - accuracy: 0.6252\n",
            "Epoch 00007: val_loss did not improve from 1.03266\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "52/52 [==============================] - 96s 2s/step - loss: 0.8639 - accuracy: 0.6227 - val_loss: 1.0915 - val_accuracy: 0.5673\n",
            "Epoch 8/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 0.8165 - accuracy: 0.6638\n",
            "Epoch 00008: val_loss improved from 1.03266 to 0.87618, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 96s 2s/step - loss: 0.8124 - accuracy: 0.6654 - val_loss: 0.8762 - val_accuracy: 0.5913\n",
            "Epoch 9/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 0.7586 - accuracy: 0.6961\n",
            "Epoch 00009: val_loss improved from 0.87618 to 0.85336, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 97s 2s/step - loss: 0.7573 - accuracy: 0.6972 - val_loss: 0.8534 - val_accuracy: 0.6346\n",
            "Epoch 10/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 0.7860 - accuracy: 0.6862\n",
            "Epoch 00010: val_loss did not improve from 0.85336\n",
            "52/52 [==============================] - 104s 2s/step - loss: 0.7922 - accuracy: 0.6825 - val_loss: 0.8577 - val_accuracy: 0.6394\n",
            "Epoch 11/50\n",
            "51/52 [============================>.] - ETA: 2s - loss: 0.8017 - accuracy: 0.6800\n",
            "Epoch 00011: val_loss improved from 0.85336 to 0.76806, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 166s 3s/step - loss: 0.8077 - accuracy: 0.6777 - val_loss: 0.7681 - val_accuracy: 0.6779\n",
            "Epoch 12/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 0.7268 - accuracy: 0.7010\n",
            "Epoch 00012: val_loss did not improve from 0.76806\n",
            "52/52 [==============================] - 68s 1s/step - loss: 0.7290 - accuracy: 0.7007 - val_loss: 0.7939 - val_accuracy: 0.6827\n",
            "Epoch 13/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.7024 - accuracy: 0.7298\n",
            "Epoch 00013: val_loss did not improve from 0.76806\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.6975 - accuracy: 0.7326 - val_loss: 0.7810 - val_accuracy: 0.6827\n",
            "Epoch 14/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.7033 - accuracy: 0.7173\n",
            "Epoch 00014: val_loss improved from 0.76806 to 0.71993, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.6956 - accuracy: 0.7216 - val_loss: 0.7199 - val_accuracy: 0.6875\n",
            "Epoch 15/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.6839 - accuracy: 0.7260\n",
            "Epoch 00015: val_loss did not improve from 0.71993\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.6867 - accuracy: 0.7241 - val_loss: 0.7219 - val_accuracy: 0.7404\n",
            "Epoch 16/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.6721 - accuracy: 0.7460\n",
            "Epoch 00016: val_loss did not improve from 0.71993\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.6728 - accuracy: 0.7436 - val_loss: 0.7323 - val_accuracy: 0.7019\n",
            "Epoch 17/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.6685 - accuracy: 0.7422\n",
            "Epoch 00017: val_loss improved from 0.71993 to 0.66491, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.6619 - accuracy: 0.7436 - val_loss: 0.6649 - val_accuracy: 0.7163\n",
            "Epoch 18/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.6776 - accuracy: 0.7435\n",
            "Epoch 00018: val_loss improved from 0.66491 to 0.66380, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.6739 - accuracy: 0.7460 - val_loss: 0.6638 - val_accuracy: 0.7308\n",
            "Epoch 19/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.6654 - accuracy: 0.7410\n",
            "Epoch 00019: val_loss improved from 0.66380 to 0.60994, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.6585 - accuracy: 0.7436 - val_loss: 0.6099 - val_accuracy: 0.7740\n",
            "Epoch 20/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.6354 - accuracy: 0.7659\n",
            "Epoch 00020: val_loss did not improve from 0.60994\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.6364 - accuracy: 0.7656 - val_loss: 0.6225 - val_accuracy: 0.7692\n",
            "Epoch 21/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.5826 - accuracy: 0.7821\n",
            "Epoch 00021: val_loss improved from 0.60994 to 0.60604, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.5799 - accuracy: 0.7814 - val_loss: 0.6060 - val_accuracy: 0.7933\n",
            "Epoch 22/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.5935 - accuracy: 0.7696\n",
            "Epoch 00022: val_loss improved from 0.60604 to 0.59340, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.5910 - accuracy: 0.7692 - val_loss: 0.5934 - val_accuracy: 0.7885\n",
            "Epoch 23/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7771\n",
            "Epoch 00023: val_loss did not improve from 0.59340\n",
            "52/52 [==============================] - 59s 1s/step - loss: 0.5674 - accuracy: 0.7790 - val_loss: 0.6393 - val_accuracy: 0.7452\n",
            "Epoch 24/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.5366 - accuracy: 0.7870\n",
            "Epoch 00024: val_loss improved from 0.59340 to 0.57837, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "52/52 [==============================] - 60s 1s/step - loss: 0.5340 - accuracy: 0.7875 - val_loss: 0.5784 - val_accuracy: 0.7788\n",
            "Epoch 25/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.5779 - accuracy: 0.7808\n",
            "Epoch 00025: val_loss did not improve from 0.57837\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.5726 - accuracy: 0.7851 - val_loss: 0.6165 - val_accuracy: 0.7788\n",
            "Epoch 26/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.5101 - accuracy: 0.8107\n",
            "Epoch 00026: val_loss did not improve from 0.57837\n",
            "52/52 [==============================] - 59s 1s/step - loss: 0.5067 - accuracy: 0.8107 - val_loss: 0.6398 - val_accuracy: 0.7548\n",
            "Epoch 27/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.5131 - accuracy: 0.8020\n",
            "Epoch 00027: val_loss did not improve from 0.57837\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.5100 - accuracy: 0.8034 - val_loss: 0.6527 - val_accuracy: 0.7644\n",
            "Epoch 28/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.5268 - accuracy: 0.8120\n",
            "Epoch 00028: val_loss did not improve from 0.57837\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.5257 - accuracy: 0.8107 - val_loss: 0.5839 - val_accuracy: 0.8029\n",
            "Epoch 29/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.5045 - accuracy: 0.8095\n",
            "Epoch 00029: val_loss improved from 0.57837 to 0.57016, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.5014 - accuracy: 0.8120 - val_loss: 0.5702 - val_accuracy: 0.7885\n",
            "Epoch 30/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4631 - accuracy: 0.8369\n",
            "Epoch 00030: val_loss did not improve from 0.57016\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.4585 - accuracy: 0.8388 - val_loss: 0.5747 - val_accuracy: 0.7692\n",
            "Epoch 31/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4904 - accuracy: 0.8232\n",
            "Epoch 00031: val_loss improved from 0.57016 to 0.54776, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.4853 - accuracy: 0.8254 - val_loss: 0.5478 - val_accuracy: 0.7981\n",
            "Epoch 32/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4600 - accuracy: 0.8481\n",
            "Epoch 00032: val_loss did not improve from 0.54776\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.4647 - accuracy: 0.8462 - val_loss: 0.6888 - val_accuracy: 0.7548\n",
            "Epoch 33/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4247 - accuracy: 0.8294\n",
            "Epoch 00033: val_loss did not improve from 0.54776\n",
            "52/52 [==============================] - 60s 1s/step - loss: 0.4267 - accuracy: 0.8266 - val_loss: 0.6415 - val_accuracy: 0.7788\n",
            "Epoch 34/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4760 - accuracy: 0.8182\n",
            "Epoch 00034: val_loss did not improve from 0.54776\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.4757 - accuracy: 0.8181 - val_loss: 0.6085 - val_accuracy: 0.7548\n",
            "Epoch 35/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4637 - accuracy: 0.8150\n",
            "Epoch 00035: val_loss did not improve from 0.54776\n",
            "52/52 [==============================] - 63s 1s/step - loss: 0.4615 - accuracy: 0.8149 - val_loss: 0.5863 - val_accuracy: 0.7885\n",
            "Epoch 36/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4334 - accuracy: 0.8406\n",
            "Epoch 00036: val_loss did not improve from 0.54776\n",
            "52/52 [==============================] - 62s 1s/step - loss: 0.4412 - accuracy: 0.8364 - val_loss: 0.6063 - val_accuracy: 0.7885\n",
            "Epoch 37/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 0.4626 - accuracy: 0.8207\n",
            "Epoch 00037: val_loss did not improve from 0.54776\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "52/52 [==============================] - 67s 1s/step - loss: 0.4652 - accuracy: 0.8217 - val_loss: 0.6231 - val_accuracy: 0.7596\n",
            "Epoch 38/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4539 - accuracy: 0.8257\n",
            "Epoch 00038: val_loss did not improve from 0.54776\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4576 - accuracy: 0.8230 - val_loss: 0.5540 - val_accuracy: 0.7692\n",
            "Epoch 39/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4360 - accuracy: 0.8356Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.54776\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4331 - accuracy: 0.8376 - val_loss: 0.5705 - val_accuracy: 0.7981\n",
            "Epoch 00039: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mKMWagP3_RE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle \n",
        "\n",
        "pickle_out = open(\"LittleVgg488_v2_history.pickle\",\"wb\")\n",
        "pickle.dump(history.history, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obg7Il0u3_RS",
        "colab_type": "code",
        "colab": {},
        "outputId": "8c3517f9-3675-4aaa-f4eb-a8ab7e935a82"
      },
      "source": [
        "epochs = 50\n",
        "\n",
        "history2 = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size,\n",
        "    initial_epoch = 39)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 52 steps, validate for 13 steps\n",
            "Epoch 40/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4337 - accuracy: 0.8394\n",
            "Epoch 00040: val_loss improved from 0.54776 to 0.51499, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4357 - accuracy: 0.8376 - val_loss: 0.5150 - val_accuracy: 0.7740\n",
            "Epoch 41/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4667 - accuracy: 0.8169\n",
            "Epoch 00041: val_loss improved from 0.51499 to 0.50129, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu_0103.h5\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4690 - accuracy: 0.8181 - val_loss: 0.5013 - val_accuracy: 0.8029\n",
            "Epoch 42/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4566 - accuracy: 0.8344\n",
            "Epoch 00042: val_loss did not improve from 0.50129\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4595 - accuracy: 0.8327 - val_loss: 0.5160 - val_accuracy: 0.7933\n",
            "Epoch 43/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 0.4632 - accuracy: 0.8244\n",
            "Epoch 00043: val_loss did not improve from 0.50129\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4603 - accuracy: 0.8254 - val_loss: 0.5704 - val_accuracy: 0.7740\n",
            "Epoch 44/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 0.4526 - accuracy: 0.8344\n",
            "Epoch 00044: val_loss did not improve from 0.50129\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4539 - accuracy: 0.8327 - val_loss: 0.5315 - val_accuracy: 0.7981\n",
            "Epoch 45/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4445 - accuracy: 0.8443\n",
            "Epoch 00045: val_loss did not improve from 0.50129\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4412 - accuracy: 0.8449 - val_loss: 0.5517 - val_accuracy: 0.7885\n",
            "Epoch 46/50\n",
            "51/52 [============================>.] - ETA: 1s - loss: 0.4787 - accuracy: 0.8150\n",
            "Epoch 00046: val_loss did not improve from 0.50129\n",
            "52/52 [==============================] - 67s 1s/step - loss: 0.4778 - accuracy: 0.8149 - val_loss: 0.5838 - val_accuracy: 0.7837\n",
            "Epoch 47/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4673 - accuracy: 0.8207\n",
            "Epoch 00047: val_loss did not improve from 0.50129\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4630 - accuracy: 0.8230 - val_loss: 0.5347 - val_accuracy: 0.7933\n",
            "Epoch 48/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4563 - accuracy: 0.8319\n",
            "Epoch 00048: val_loss did not improve from 0.50129\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4613 - accuracy: 0.8291 - val_loss: 0.5273 - val_accuracy: 0.7788\n",
            "Epoch 49/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.4908 - accuracy: 0.8144Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.50129\n",
            "52/52 [==============================] - 66s 1s/step - loss: 0.4903 - accuracy: 0.8144 - val_loss: 0.5301 - val_accuracy: 0.7981\n",
            "Epoch 00049: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNOgcM8y3_RV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle_out = open(\"LittleVgg488_v2_history2.pickle\",\"wb\")\n",
        "pickle.dump(history2.history, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmDpcHbc3_Ra",
        "colab_type": "code",
        "colab": {},
        "outputId": "440ea710-1f40-4586-ab82-9e9cb360da94"
      },
      "source": [
        "epochs = 50\n",
        "\n",
        "history3 = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size,\n",
        "    initial_epoch = 38)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 59 steps, validate for 15 steps\n",
            "Epoch 39/50\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5423 - accuracy: 0.8105\n",
            "Epoch 00039: val_loss improved from 0.51419 to 0.50634, saving model to C:/Users/thisi/Documents/Shared-CV/YarraValley/models/multi_littleVgg_gpu2.h5\n",
            "59/59 [==============================] - 73s 1s/step - loss: 0.5385 - accuracy: 0.8116 - val_loss: 0.5063 - val_accuracy: 0.8000\n",
            "Epoch 40/50\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7800\n",
            "Epoch 00040: val_loss did not improve from 0.50634\n",
            "59/59 [==============================] - 72s 1s/step - loss: 0.5697 - accuracy: 0.7784 - val_loss: 0.5252 - val_accuracy: 0.7875\n",
            "Epoch 41/50\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5760 - accuracy: 0.7832\n",
            "Epoch 00041: val_loss did not improve from 0.50634\n",
            "59/59 [==============================] - 71s 1s/step - loss: 0.5698 - accuracy: 0.7859 - val_loss: 0.5126 - val_accuracy: 0.8000\n",
            "Epoch 42/50\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5616 - accuracy: 0.7877\n",
            "Epoch 00042: val_loss did not improve from 0.50634\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "59/59 [==============================] - 70s 1s/step - loss: 0.5663 - accuracy: 0.7850 - val_loss: 0.5456 - val_accuracy: 0.8000\n",
            "Epoch 43/50\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5831 - accuracy: 0.7876\n",
            "Epoch 00043: val_loss did not improve from 0.50634\n",
            "59/59 [==============================] - 72s 1s/step - loss: 0.5813 - accuracy: 0.7869 - val_loss: 0.5260 - val_accuracy: 0.8375\n",
            "Epoch 44/50\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5451 - accuracy: 0.7963Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.50634\n",
            "59/59 [==============================] - 73s 1s/step - loss: 0.5407 - accuracy: 0.7987 - val_loss: 0.5365 - val_accuracy: 0.7667\n",
            "Epoch 00044: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcA74G5c3_Rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle_out = open(\"LittleVgg488_history3.pickle\",\"wb\")\n",
        "pickle.dump(history3.history, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqU6eDpI3_R1",
        "colab_type": "code",
        "colab": {},
        "outputId": "cad09cc4-1b6f-4319-b072-2ef000411ace"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# We need to recreate our validation generator with shuffle = false\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset = 'validation',\n",
        "        shuffle=False)\n",
        "\n",
        "\n",
        "class_labels = validation_generator.class_indices\n",
        "class_labels = {v: k for k, v in class_labels.items()}\n",
        "classes = list(class_labels.values())\n",
        "\n",
        "# nb_train_samples = 966\n",
        "nb_validation_samples = 220\n",
        "\n",
        "#Confution Matrix and Classification Report\n",
        "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size +1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(validation_generator.classes, y_pred))\n",
        "print('Classification Report')\n",
        "target_names = list(class_labels.values())\n",
        "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
        "\n",
        "plt.imshow(cnf_matrix, interpolation='nearest')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
        "_ = plt.yticks(tick_marks, classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 220 images belonging to 3 classes.\n",
            "Confusion Matrix\n",
            "[[64 12  1]\n",
            " [ 4 67  4]\n",
            " [ 2 23 43]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  burst_blue       0.91      0.83      0.87        77\n",
            "     nothing       0.66      0.89      0.76        75\n",
            "   stop_gold       0.90      0.63      0.74        68\n",
            "\n",
            "    accuracy                           0.79       220\n",
            "   macro avg       0.82      0.79      0.79       220\n",
            "weighted avg       0.82      0.79      0.79       220\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAHVCAYAAAA3qhg4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3debRmVXnn8e+PSQRBhhKCEgMSiFFBVBRpaEUwhkSjLMEoMWnIIot26ES7Y0eS7gQ1rlYSNVETBxKjZRwRNRBiRJsG50CKQUDQ0A1IDASCDCJBhrpP//Gektdr1b2XqveeU+fu72ets+57hnefB69VTz1777NPqgpJkjQOWwwdgCRJWjoTtyRJI2LiliRpREzckiSNiIlbkqQR2WroACRJ6tPPP2v7+u6ta2fe7kWX3XNOVR0184bnMXFLkpry3VvXcuE5j555u1vucfWqmTe6HiZuSVJTCphjbugwNppj3JIkjYgVtySpMcXasuKWJEk9sOKWJDVlMsY93vd0WHFLkjQiVtySpOaMeVa5iVuS1JSiWDviV1rbVS5J0ohYcUuSmuPkNEmS1AsrbklSUwpYO+KK28QtSWqOXeWSJKkXVtySpKYU+DiYJEnqhxW3JKk54103zcQtSWpMUaOeVW5XuSRJI2LFLUlqS8Ha8RbcVtySJI2JFbckqSnFuCenWXFLkjQiVtySpMaEtWToIDaaiVuS1JQC5pycJkmS+mDFLUlqzpi7yq24JUkaEStuSVJTinFX3CZuSVJz5mq8iduuckmSRsSKW5LUlLF3lVtxS5I0IlbckqSmFGHtiOtWE7ckqTlOTpMkSb2w4pYkNWXsk9NM3FN22WWL2nPPLYcOQ8vgumt2GzoELae77h46Ai2DH3AX99Y9482wy8TEPWXPPbfk7M+sGjoMLYMTf/llQ4eg5fQPlw0dgZbBBXXuMrUc1tZ4R4rHG7kkSQ2y4pYkNaWAuRHXreONXJKkjbSWzHxbiiQ7JTkjyTeTXJXkkCS7JPl8kqu7nzsv1IaJW5Kk/rwd+GxVPRZ4InAVcDJwblXtC5zb7W+QXeWSpKZUDTM5LcmOwDOAEyZx1L3AvUleABzeXbYaOB947YbaseKWJGk2ViVZM7WdNO/8Y4B/A96f5JIkf5lke2D3qroRoPu54POrVtySpObMLc8CLLdU1UELnN8KeDLwm1V1QZK3s0i3+IYakSSpGZOV0wbpcP4O8J2quqDbP4NJ4r4pyR5VdWOSPYCbF2rErnJJknpQVf8K/HOSn+kOHQlcCZwFHN8dOx44c6F2rLglSY0ZdOW03wQ+nGQb4Brg15kU0acnORG4HnjRQg2YuCVJ6klVXQqsbxz8yKW2YeKWJDVl7CunmbglSc1ZW+N96dh4/8khSVKDrLglSU0pMtTjYDMx3sglSWqQFbckqTlzwz0OtsnGG7kkSQ2y4pYkNWXAJU9nwsQtSWpKER8HkyRJ/bDiliQ1Z8wrp403ckmSGmTFLUlqShVDvh1sk5m4JUmNCXM4OU2SJPXAiluS1JRi3F3l441ckqQGWXFLkprjymmSJI1EEeZcOU2SJPXBiluS1Jwxd5WPN3JJkhpkxS1JakoBcz4OJkmS+mDFLUlqTFg74iVPTdySpKbYVS5JknpjxS1Jas6Yu8qtuCVJGhErbklSU6oy6jFuE7ckqTm+1lOSJPXCiluS1JQC5pycJkmS+mDFLUlqTEY9xm3iliQ1ZbJyml3lkiSpB1bckqTmrB1x3TreyCVJapAVtySpKUUc45YkSf2w4pYkNWduxHWriVuS1JQqWGtXuSRJ6oMVtySpOSt6clqSvZJcMcubJtkpySsWuebwJGdv4Nx1SVbNMiZJksZgWbvKk2yoot8JWDBxS5K0HCaPg20x860vS73TVklWJ7ksyRlJtpuuepMclOT87vPrkpyW5HPAB5M8PsmFSS7tvr8v8GZgn+7YHy9w3x2TfDrJlUnek+RH4p3fG5DkNUle133eJ8lnk1yU5EtJHru+GyQ5KcmaJGtuvXVuif9zSJLGbC2Z+daXpY5x/wxwYlV9JclfsXi1/BTgsKq6O8k7gbdX1YeTbANsCZwMPKGqDlyknacBjwO+DXwWeCFwxhJjPg14WVVdneRg4F3AEfMvqqrTums54ICta4ltS5I0iKUm7n+uqq90nz8E/NYi159VVXd3n78G/I8kewKf6hLpUuO7sKquAUjyUeAwlpC4kzwM+A/AJ6bu9ZCl3lSStHKN/e1gS03c8yvRAu7nga72beedv+uHF1Z9JMkFwHOBc5L8BnDNJtx32nQM03FsAdy+hIpekqRRWeoY96OTHNJ9Pg74MnAdky5xgGM29MUkjwGuqap3AGcBBwB3Ajss4b5PS7J3N7b94u6+024Cdkuya5KHAM8DqKrvAdcmeVEXQ5I8cQn3kySteG1MTrsKOD7JZcAuwLuB1wNvT/IlYO0C330xcEWSS4HHAh+squ8CX0lyxSKT077GZCLbFcC1wKenT1bVfcAbgAuAs4FvTp1+KXBikq8D3wBesMT/VknSCjdHZr71ZdGu8qq6jskEsfm+BOy3nutfN2//TcCb1nPdryxy3/OB8zdwbq+pz+8A3rGea64FjlroHpIkjY0rp0mSmjL2tcoHT9xJ9gf+et7he6rq4CHikSRpczZ44q6qywFnf0uSetPnZLJZGzxxS5LUiiTXMXmyai1wf1UdlGQX4OPAXkye2PrlqrptQ22M958ckiRthMla5bPfHoRnVdWBVXVQt38ycG5V7Quc2+1vkIlbktSczexxsBcAq7vPq4GjF7rYxC1J0mysWvfSqm47aT3XFPC57gVY687vXlU3AnQ/d1voJo5xS5Kasoxrld8y1f29IYdW1Q1JdgM+n+Sbi1z/Y6y4JUnqSVXd0P28mclqoE8DbkqyB0D38+aF2jBxS5KaM8Ra5Um2T7LDus/Ac5gs6X0WcHx32fHAmQu1Y1e5JKktD34W+KzsDny6e930VsBHquqzSf4ROD3JicD1wIsWasTELUlSD6rqGuDH3lTZvXjryKW2Y+KWJDWloNe3ec2aY9ySJI2IFbckqTkDjXHPhIlbktSUZXyOuxd2lUuSNCJW3JKk5lhxS5KkXlhxS5Kasu61nmNlxS1J0ohYcUuSmjPmBVhM3JKktpST0yRJUk+suCVJTXEBFkmS1BsrbklSc8ZccZu4JUlN8TluSZLUGytuSVJzyopbkiT1wYpbktQcV06TJGkkypXTJElSX6y4JUnNcXKaJEnqhRW3JKkx416AxcQtSWqOXeWSJKkXVtySpKb4Wk9JktQbK25JUltqsgjLWFlxS5I0IlbckqTmuFa5JEkjUfg4mCRJ6okVtySpMeNeOc2KW5KkEbHiliQ1Z8yPg5m4JUnNcXKaJEnqhRX3lG9fuRMnHfDcocPQMjjnyg8OHYKW0S8+7plDh6BlkO9tuSztVllxS5KknlhxS5KaM+bHwUzckqTmjHlWuV3lkiSNiBW3JKk5Tk6TJEm9sOKWJDWliBW3JEnqhxW3JKk5I55UbuKWJDXGldMkSVJfrLglSe0ZcV+5FbckSSNixS1Jas6Yx7hN3JKk5rhWuSRJ6oUVtySpKcW4u8qtuCVJ6kmSLZNckuTsbn/vJBckuTrJx5Nss1gbJm5JUlsKqMx+W5pXAVdN7Z8K/ElV7QvcBpy4WAMmbklSc6pmvy0myZ7Ac4G/7PYDHAGc0V2yGjh6sXZM3JIkzcaqJGumtpPmnf9T4HeAuW5/V+D2qrq/2/8O8KjFbuLkNElSe5bncbBbquqg9Z1I8jzg5qq6KMnh6w5vTGQmbkmSlt+hwPOT/CKwLbAjkwp8pyRbdVX3nsANizVkV7kkqTGhavbbQqrqd6tqz6raC3gJ8H+q6qXAecCx3WXHA2cuFr2JW5Kk4bwW+G9J/i+TMe/3LfYFu8olSe0ZcMnTqjofOL/7fA3wtAfzfRO3JKkt5cppkiSpJ1bckqT2+HYwSZLUBytuSVKDxjvGbeKWJLXHrnJJktQHK25JUnusuCVJUh+suCVJbSlgxAuwmLglSc0pu8olSVIfrLglSe2x4pYkSX2w4pYktWfEk9OsuCVJGhErbklSczLiMW4TtySpLYWT0yRJUj+suCVJjYmT0yRJUj+suCVJ7RnxGLeJW5LUnhEnbrvKJUkaEStuSVJ7rLglSVIfrLglSW0pRv04mIlbktScMS95ale5JEkjYsUtSWqPFbckSerDZpO4k5yQ5JFT+9clWbWe656f5OR+o5MkafOwOXWVnwBcAdyw0EVVdRZwVh8BSZK0uVm2ijvJXkmuSvIXSb6R5HNJHprkwCT/kOSyJJ9OsnOSY4GDgA8nuTTJQ7tmfjPJxUkuT/LYrt0TkvxZ9/kDSd6R5KtJrunaIckWSd7V3ffsJJ9Zd06SpNTst74sd1f5vsCfV9XjgduBY4APAq+tqgOAy4FTquoMYA3w0qo6sKru7r5/S1U9GXg38JoN3GMP4DDgecCbu2MvBPYC9gd+AzhkQwEmOSnJmiRr7v3hbSVJK1pl9ltPljtxX1tVl3afLwL2AXaqqi90x1YDz1jg+5+a+u5eG7jmb6pqrqquBHbvjh0GfKI7/q/AeRu6QVWdVlUHVdVB2/yw0JckafO03GPc90x9XgvstJHfX8uGY52+R+b9lCTpRxU+DvYg3AHcluQ/dvu/Bqyrvu8EdpjRfb4MHNONde8OHD6jdiVJGtQQs8qPB96TZDvgGuDXu+Mf6I7fzQJj0kv0SeBIJrPU/wm4gMk/GiRJGnXFvWyJu6quA54wtf+WqdNPX8/1n2SScNfZa+rcGrqquao+wCTJU1UnzGvjYd3PuSSvqarvJ9kVuJDJRDhJkka9Vvnm9Bz3rJ2dZCdgG+APu0lqkiSN2opN3FV1+NAxSJI2UyOuuDebJU8lSdLiVmzFLUnSBo244jZxS5Ka0vcSpbNmV7kkSSNixS1Jak+Pa4vPmhW3JEkjYsUtSWqPY9ySJKkPVtySpOaMeVa5iVuS1J4RJ267yiVJGhErbklSW1yARZIk9cWKW5LUnhFX3CZuSVJ7Rpy47SqXJKkHSbZNcmGSryf5RpLXd8f3TnJBkquTfDzJNgu1Y+KWJDVn3RvCZrktwT3AEVX1ROBA4KgkTwdOBf6kqvYFbgNOXKgRE7ckST2oie93u1t3WwFHAGd0x1cDRy/UjolbkqTZWJVkzdR20vwLkmyZ5FLgZuDzwP8Dbq+q+7tLvgM8aqGbODlNktSe5ZmcdktVHbTgbavWAgcm2Qn4NPCz67tsoTasuCVJ6llV3Q6cDzwd2CnJukJ6T+CGhb5r4pYktWUZJqYtZXJakkd0lTZJHgo8G7gKOA84trvseODMhdqxq1ySpH7sAaxOsiWTwvn0qjo7yZXAx5K8EbgEeN9CjZi4JUntGWABlqq6DHjSeo5fAzxtqe3YVS5J0ohYcUuS2jPiJU9N3JKkpgRf6ylJknpixS1Jao8VtyRJ6oMVtySpLUt/m9dmycQtSWrPiBO3XeWSJI2IFbckqT1W3JIkqQ9W3JKk5jg5TZKkMRlx4rarXJKkEbHiliS1pbDiliRJ/bDiliQ1Z8yT06y4JUkaEStuSVJ7Rlxxm7glSc2xq1ySJPXCiluS1B4rbkmS1AcrbklSW0a+AIuJW5LUlHTbWNlVLknSiFhxT6m5tcx9/66hw9AyeMrrXj50CFpG977/jqFD0DK497e3Xr7GR9xVbsUtSdKIWHFLkpoz5gVYTNySpPaMOHHbVS5J0ohYcUuS2mPFLUmS+mDFLUlqSzk5TZKkcRlx4rarXJKkEbHiliQ1Z8xd5VbckiSNiBW3JKk9VtySJKkPVtySpOaMeYzbxC1JakthV7kkSeqHFbckqT1W3JIkqQ9W3JKkpgQnp0mSNC4jTtx2lUuSNCJW3JKk5qTGW3JbcUuSNCJW3JKktox8ARYTtySpOWOeVW5XuSRJI2LFLUlqjxW3JEnqgxW3JKk5jnFLkqRemLglSe2pZdgWkeQnk5yX5Kok30jyqu74Lkk+n+Tq7ufOC7Vj4pYktaUmXeWz3pbgfuC3q+pngacDr0zyOOBk4Nyq2hc4t9vfIBO3JEk9qKobq+ri7vOdwFXAo4AXAKu7y1YDRy/UjpPTJEntWZ7JaauSrJnaP62qTlvfhUn2Ap4EXADsXlU3wiS5J9ltoZuYuCVJmo1bquqgxS5K8jDgk8Crq+p7SR7UTUzckqSmhOEeB0uyNZOk/eGq+lR3+KYke3TV9h7AzQu14Ri3JKk9VbPfFpFJaf0+4KqqetvUqbOA47vPxwNnLtSOFbckSf04FPg14PIkl3bHfg94M3B6khOB64EXLdSIiVuS1Jwhusqr6stMeurX58iltmNXuSRJI2LFLUlqyxJXOttcmbglSc3J3NARbDy7yiVJGhErbklSe0bcVW7FLUnSiFhxS5KaM9TKabNgxS1J0ohYcUuS2lIsaYnSzZWJW5LUHLvKJUlSL6y4JUntseKWJEl9sOKWJDUljHuM28QtSWpL1ahnldtVLknSiFhxS5KaM+aucituSZJGxIpbktSe1ivuJK9Ost0s2tqEGA5PcvYGzl2XZFXfMUmSNk+p2W99mVVX+auBQRO3JEkteNCJO8n2Sf4uydeTXJHkFOCRwHlJzuuuOS7J5d35U6e++/0kb01ycZJzkzxigfs8NcllSb6W5I+TXNEd3zbJ+7v2L0nyrPV8d9ckn+vOv5fJY3sbus9JSdYkWXNf3fNg/+eQJI1NAXM1+60nG1NxHwXcUFVPrKonAH8K3AA8q6qeleSRwKnAEcCBwFOTHN19d3vg4qp6MvAF4JQF7vN+4GVVdQiwdur4KwGqan/gOGB1km3nffcU4MtV9STgLODRG7pJVZ1WVQdV1UFb5yFL+e+XJGkwG5O4LweeneTUJP+xqu6Yd/6pwPlV9W9VdT/wYeAZ3bk54OPd5w8Bh63vBkl2Anaoqq92hz4ydfow4K8BquqbwLeB/eY18Yyufarq74DbHtx/oiRpRatl2HryoGeVV9U/JXkK8IvAm5J8bt4lG+yWXl9zGzi+UBtLbX/EcwYlSVq/jRnjfiTw71X1IeAtwJOBO4EduksuAJ6ZZFWSLZl0Z39h6n7Hdp9/Bfjy+u5RVbcBdyZ5enfoJVOnvwi8tItlPybd4N+a18T0Nb8A7Pxg/zslSSvXmGeVb8xz3PsDf5xkDrgPeDlwCPD3SW7sxrl/FziPSXX8mao6s/vuXcDjk1wE3AG8eIH7nAj8RZK7gPO76wHeBbwnyeXA/cAJVXVP8iOF+OuBjya5mMk/Gq7fiP9OSdJKNeK1yjemq/wc4Jx5h9cA75y65iP86Lj09Pd/H/j9JdzqG1V1AECSk7t7UFU/AE5YT7vnM0nwVNV3gedMnf6vS7ifJEmbvc155bTndpX7VkwmoJ0wbDiSpJVizGuV95q4q+ph848l+XPg0HmH315V7+eBGeiSJInNoOKuqlcOHYMkqSE9P741a4MnbkmS+hQgI56c5ms9JUkaEStuSVJ75oYOYONZcUuSNCJW3JKk5ox5jNvELUlqy8hnldtVLknSiFhxS5IaU6Neq9yKW5KkEbHiliQ1Z8xrlVtxS5I0IlbckqT2jHiM28QtSWpLQVw5TZIk9cGKW5LUnhF3lVtxS5I0IlbckqT2jLfgNnFLktoz5peM2FUuSdKIWHFLktpjxS1JkvpgxS1JaksBI16AxcQtSWpKKCenSZKkflhxS5LaY8UtSZL6YOKWJLWnavbbIpL8VZKbk1wxdWyXJJ9PcnX3c+fF2jFxS5LUjw8AR807djJwblXtC5zb7S/IxC1Jasu6x8FmvS1226ovArfOO/wCYHX3eTVw9GLtODlNktScZXocbFWSNVP7p1XVaYt8Z/equhGgqm5MsttiNzFxS5I0G7dU1UHLfRMTtySpPZvP42A3Jdmjq7b3AG5e7AuOcUuSNJyzgOO7z8cDZy72BStuSVJjlvb41qwl+ShwOJOx8O8ApwBvBk5PciJwPfCixdoxcUuS2lIMkrir6rgNnDrywbRjV7kkSSNixS1Jas+IX+tpxS1J0ohYcUuSmjPm93GbuCVJ7Rlx4rarXJKkEbHiliS1pYA5K25JktQDK25JUmOGWTltVqy4JUkaEStuSVJ7Rlxxm7glSe0ZceK2q1ySpBGx4pYktWXkj4OZuKfcWbfd8vn7PvbtoePoySrglqGD6M17PzZ0BH1q63cL8N6hA+hNa7/bnxo6gM2RiXtKVT1i6Bj6kmRNVR00dByaPX+3K5e/21kpqPG+HszELUlqj5PTJElSH6y423Xa0AFo2fi7Xbn83c7CyCenWXE3qqr8C2CF8ne7cvm7FVhxS5JaNOIxbhO3JKk9I07cdpVLkjQiVtySpMaM+7WeJm5pBUjywvUcvgO4vKpu7jsebbokuyx0vqpu7SsWbV5M3I1Jsh/wbmD3qnpCkgOA51fVGwcOTZvmROAQ4Lxu/3DgH4D9kryhqv56qMC00S5i8uBSgEcDt3WfdwKuB/YeLrSRK2BuvCunOcbdnr8Afhe4D6CqLgNeMmhEmoU54Ger6piqOgZ4HHAPcDDw2kEj00apqr2r6jHAOcAvVdWqqtoVeB7wqWGjWwGqZr/1xMTdnu2q6sJ5x+4fJBLN0l5VddPU/s3Afl136n0DxaTZeGpVfWbdTlX9PfDMAePRwOwqb88tSfZh0llEkmOBG4cNSTPwpSRnA5/o9o8Bvphke+D24cLSDNyS5H8CH2Ly5/ZXge8OG9IK4OQ0jcgrmSyb+Ngk/wJcy+QvAo3bK5kk60OZjIN+EPhkVRXwrCED0yY7DjgF+HS3/8XumBpl4m5MVV0DPLurxLaoqjuHjkmbrkvQZ3SbVpBuuONVQ8exstSo1yo3cTcmyR/M2wegqt4wSECaie5xsFOB3ZhU3GGSz3ccNDBttCR/SzektT5V9fwew9FmxMTdnrumPm/LZIbqVQPFotn5IyYzj/1drhxvGTqAFaugaryPg5m4G1NVb53eT/IW4KyBwtHs3GTSXlmq6gvrPifZBtiv2/1WVfmkwKayq1wjth3wmKGD0CZbk+TjwN8weX4bgKryed+RS3I4sBq4jskQyE8mOb6qvjhkXBqOibsxSS7ngXGzLYFHAI5vj9+OwL8Dz5k6VrhQx0rwVuA5VfUt+OHqhx8FnjJoVGPn42AakedNfb6fSRerC7CMXFX9+tAxaNlsvS5pA1TVPyXZesiANCwTdyOmXlgw//GvHZP4woKRSvI7VfVHSd7JemYgV9VvDRCWZmtNkvcB69abfymTdcy1sapGvVa5ibsd0y8smK9wnHus1k1IWzNoFFpOL2eywM5vMfnz+0XgXYNGtBLYVa7NXVX5JqEVqKr+tvu5euhYtDyq6h7gbd0mmbhb1C3WcRiTSvtLVfU3A4ekTdRNWHoNsBdTf66r6oihYtJszJtQus4dTHpZ3lhVrlu+Ecquco1FkncBP81kVirAy5L8XFW9csCwtOk+AbwH+Etg7cCxaLb+nsnv9CPd/kuYdJnfAXwA+KVhwtJQTNzteSbwhG5ta5KsBi4fNiTNwP1V9e6hg9CyOLSqDp3avzzJV6rq0CS+IGij9Pv+7Fnzfdzt+Rbw6Kn9nwQuGygWbaIku3RPDPxtklck2WPdsaknCTRuD0ty8LqdJE8DHtbt+ijnxigmK6fNeuuJFXcjpl5Y8HDgqiQXdvsHA18dMjZtkvlPC/z3qXM+LbAy/AbwV0nWJes7gRO7N/y9abiwNBQTdzt8YcEKtO5pgSTbVtUPps8l2XaYqDRLVfWPwP5JHg6kqm6fOn16t/ypTxU8WL5kRJu76RcWLCTJ16rqkOWORzP3VeDJSzimkaqqOzZw6lVM1jJXI0zcms8qbUSS/ATwKOChSZ7EA13mOzJ5gYxWvvUtqqQFFFC+HUwryHj/39ymnwdOAPbkRxfouBP4vSECUu/8M9sYE7c0Yt3Y5uokx1TVJ4eOR4Ow4n6wqhzj1oriXwLjdG6StwHP6Pa/ALxhgXFRrRxfGTqAMRpzV7nPcTcmyamLHPu1HsPR7LyPSff4L3fb94D3DxqRZiLJrknemeTiJBcleXuSXdedr6r/MmR86p+Juz0/t55jv7DuQ1Vd0WMsmp19quqUqrqm216Pz3CvFB8DbgaOAY4F/g34+KARrQQ1N/utJ3aVNyLJy4FXAI9JMr1S2g7Y1bYS3J3ksKr6MkCSQ4G7B45Js7FLVf3h1P4bkxw9WDQrwJ3cds7/rjNWLUPTtyxDmz8mNeL1WrV03eINOzNZaenkqVN3VtWtw0SlWUlyIJNneR/eHboNOL6qXM525JK8hcmbwE7vDh0LPL6qThkuKg3JxN2YJPsA36mqe5IcDhwAfHDeakwamSQPYfIX+j7ATkzeHFVV9YZBA9MmS3InsD2wri92C+Cu7nNV1Y6DBKbBOMbdnk8Ca5P8NJMJTXvzwOsCNV5nMnm94w+AfwG+zwN/uWvEqmqHqtqiqrbqti26YzuYtNvkGHd75qrq/iQvBP60qt6Z5JKhg9Im27Oqjho6CC2PJM/ngUf9zq+qs4eMR8Oy4m7PfUmOA/4TsO4P/9YDxqPZ+GqS/YcOQrOX5M1M1iO/stte1R1ToxzjbkySxwEvA75WVR9Nsjfw4qryL4IRS3Il8NPAtcA9TBbSqao6YNDAtMm6p0AOrJo8b5RkS+ASf7ftsqu8Id0f+N+rql9dd6yqrgVM2uP3C4tfohHbCVj39MfDF7pQK5+JuyFVtTbJI5JsU1X3Dh2PZqeqvj10DFo2bwIuSXIek56UZ+ALZJpmV3ljkryXyTuaz2Jq1nFVvW2DX5I0qCR7AE9lkrgvqKp/HTgkDciKuz03dNsWTFZNk7QZS3JuVR3J5B/b84+pQSbuxnRrWEvazCXZFtgOWJVkZx54c9+OwCMHC0yDM3E3phsn+7Hxkao6YoBwJG3YfwZezSRJX0T3pACTt8D92YBxaWCOcTcmyVOmdrdl8sah+6vqdwYKSdICkvwBk8WSvpfk95nMUfnDqrp44NA0EBO3SPKFqnrm0HFI+nFJLquqA5IcBvwv4K1MHp3Oi3EAAAD/SURBVOs8eODQNBBXTmtMkl2mtlVJjgJ+Yui4JG3Q2u7nc4H3VNWZwDYDxqOBOcbdnot4YIz7fuA64MTBopG0mH/pHuN8NnBq9yY4i66G2VXemCQPBV4BHMYkgX8JeHdV/WDQwCStV5LtgKOAy6vq6u6Z7v2r6nMDh6aBmLgbk+R04HvAh7tDxwE7V9WLhotKkrRUJu7GJPl6VT1xsWOSpM2T4yTtuSTJ09ftJDkY+MqA8UiSHgQr7kYkuZzJmPbWwM8A13f7PwVcWVVPGDA8SdISmbgbkeSnFjrv26UkaRxM3JIkjYhj3JIkjYiJW5KkETFxS5I0IiZuSZJG5P8DxsoQhWpFSjkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}